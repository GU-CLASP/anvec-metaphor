{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Composing adjective-noun into metaphor-literal vectors space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import numpy as np\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load word embeddings\n",
    "# you can try to add more pretrained word embeddings in out collection\n",
    "# but just loading each file into memory is a time consuming process. \n",
    "# Loading them all together is not recommended. \n",
    "embeddings = {\n",
    "    'w2v-gnews': models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Adjective-Noun compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "adjectives 23\n",
      "nouns      3418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the file and filter those who are not in the embeddings\n",
    "phrase_annotate = []\n",
    "with open('AN-phrase-annotations.csv') as f_csv:\n",
    "    for i, line in enumerate(f_csv):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        \n",
    "        adj, noun, is_meta, count = line.strip().split(',')\n",
    "        \n",
    "        is_oov = False\n",
    "        for title, emb in embeddings.items():\n",
    "            if noun[:-2] not in emb or adj[:-2] not in emb:\n",
    "                is_oov = True\n",
    "\n",
    "        if is_oov:\n",
    "            continue\n",
    "        \n",
    "        phrase_annotate.append((\n",
    "            adj[:-2],\n",
    "            noun[:-2],\n",
    "            1 if is_meta=='y' else 0,\n",
    "            0 if count=='#N/A' else int(count))\n",
    "        )\n",
    "\n",
    "adjectives = set(adj for adj, _, _, _ in phrase_annotate)\n",
    "nouns = set(n for _, n, _, _ in phrase_annotate)\n",
    "\n",
    "print(\"\"\"\n",
    "{0:10} {nadj}\n",
    "{1:10} {nn}\n",
    "\"\"\".format('adjectives', 'nouns', nadj=len(adjectives),nn=len(nouns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.layers import Input, Flatten, Reshape, Merge, Lambda, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffel the training data:\n",
    "phrase_annotate_org = phrase_annotate[:]\n",
    "np.random.shuffle(phrase_annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of composing vectors is similar to Mitchell and Lapata (2010):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{p} = f(\\mathbf{u}, \\mathbf{v}; \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where, $\\mathbf{u}$ and $\\mathbf{v}$ are two word vector representations to be composed, and $\\mathbf{p}$ is the vector representation of their composition. Then function $f$ parameterized by $\\theta$ (a list of parameters to be learned). Based on Mitchel and Lapata (2008) it must contain latent information about the syntactic information about the composition and world knowledge about each word (or what ever it represents).\n",
    "\n",
    "The objective of our model is to learn parameters of this funciton where the phrase vector representations for metaphoric compositions can be disambiguiated from literal compositions. For this matter, we proposed a neural network which learns this representaitons in a hidden layer. \n",
    "\n",
    "As a result, the final layer before prediction of literal and metaphoric will be considered as compositional representation. The final weight matrix in this model will be a vector in same space where indicates the maximal metaphoricity, where basically degree of metaphoricity can be compared by cosine similarity with this vector.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@article{mitchell2010composition,\n",
    "  title={Composition in distributional models of semantics},\n",
    "  author={Mitchell, Jeff and Lapata, Mirella},\n",
    "  journal={Cognitive science},\n",
    "  volume={34},\n",
    "  number={8},\n",
    "  pages={1388--1429},\n",
    "  year={2010},\n",
    "  publisher={Wiley Online Library}\n",
    "}\n",
    "\n",
    "@inproceedings{mitchell2008vector,\n",
    "  title={Vector-based Models of Semantic Composition.},\n",
    "  author={Mitchell, Jeff and Lapata, Mirella},\n",
    "  booktitle={ACL},\n",
    "  pages={236--244},\n",
    "  year={2008}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### First Architecture\n",
    "\n",
    "One possible formulation is similar to additive composition in Mitchel and Lapata (2010), but instead of scalar modification of each vecotr, a weight matrix scales each feature dimention and additive regulirizer is used to avoid overfitting:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{p} = \\mathbf{u}W_{adjective} + \\mathbf{v} W_{noun} + b \\\\\n",
    "W = \\left[\\begin{array}{l}\n",
    "      W_{adjective} \\\\\n",
    "      W_{noun}\n",
    "    \\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $b$ is the regularization, the composition function with $\\theta = (W, b)$ follows the Michel and Lapatas formulation. This formulation is very similar to composition model in Socher et al. (2011) and (2012), where the non-linearity funciton $g$, instead we use a linear identity with a regulirizer:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{p} = f_{\\theta}(\\mathbf{u}, \\mathbf{v}) = [\\mathbf{u} ; \\mathbf{v}] W + b\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@inproceedings{socher2012semantic,\n",
    "  title={Semantic compositionality through recursive matrix-vector spaces},\n",
    "  author={Socher, Richard and Huval, Brody and Manning, Christopher D and Ng, Andrew Y},\n",
    "  booktitle={Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},\n",
    "  pages={1201--1211},\n",
    "  year={2012},\n",
    "  organization={Association for Computational Linguistics}\n",
    "}\n",
    "@inproceedings{socher2011semi,\n",
    "  title={Semi-supervised recursive autoencoders for predicting sentiment distributions},\n",
    "  author={Socher, Richard and Pennington, Jeffrey and Huang, Eric H and Ng, Andrew Y and Manning, Christopher D},\n",
    "  booktitle={Proceedings of the conference on empirical methods in natural language processing},\n",
    "  pages={151--161},\n",
    "  year={2011},\n",
    "  organization={Association for Computational Linguistics}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 2s - loss: 0.5874 - acc: 0.7280 - recall: 0.7533 - precision: 0.7234     \n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 0s - loss: 0.3618 - acc: 0.8840 - recall: 0.8640 - precision: 0.8996     \n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 0s - loss: 0.2507 - acc: 0.9240 - recall: 0.9369 - precision: 0.9127     \n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 0s - loss: 0.1900 - acc: 0.9400 - recall: 0.9696 - precision: 0.9181     \n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 0s - loss: 0.1494 - acc: 0.9600 - recall: 0.9749 - precision: 0.9447     \n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 0s - loss: 0.1215 - acc: 0.9680 - recall: 0.9761 - precision: 0.9601     \n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 0s - loss: 0.1032 - acc: 0.9780 - recall: 0.9964 - precision: 0.9606     \n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 0s - loss: 0.0874 - acc: 0.9840 - recall: 0.9962 - precision: 0.9728     \n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 0s - loss: 0.0757 - acc: 0.9880 - recall: 0.9958 - precision: 0.9811     \n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 0s - loss: 0.0658 - acc: 0.9920 - recall: 0.9962 - precision: 0.9881     \n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 0s - loss: 0.0568 - acc: 0.9940 - recall: 0.9953 - precision: 0.9929     \n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 0s - loss: 0.0503 - acc: 0.9940 - recall: 0.9955 - precision: 0.9923     \n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 0s - loss: 0.0441 - acc: 0.9960 - recall: 0.9965 - precision: 0.9959     \n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 0s - loss: 0.0395 - acc: 0.9960 - recall: 0.9955 - precision: 0.9955     \n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 0s - loss: 0.0357 - acc: 0.9960 - recall: 0.9950 - precision: 0.9960     \n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 0s - loss: 0.0317 - acc: 0.9980 - recall: 0.9958 - precision: 1.0000     \n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 0s - loss: 0.0281 - acc: 1.0000 - recall: 1.0000 - precision: 1.0000     \n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 0s - loss: 0.0252 - acc: 1.0000 - recall: 1.0000 - precision: 1.0000     \n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 0s - loss: 0.0229 - acc: 1.0000 - recall: 1.0000 - precision: 1.0000     \n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 0s - loss: 0.0209 - acc: 1.0000 - recall: 1.0000 - precision: 1.0000     \n",
      "7992/7992 [==============================] - 0s\n",
      "\n",
      "\n",
      "Embedding: w2v-gnews\n",
      "acc        0.9358\n",
      "recall     0.9433\n",
      "loss       0.1832\n",
      "precision  0.9374\n"
     ]
    }
   ],
   "source": [
    "# First choose an embedding for this part\n",
    "# embeding {title, total-score, per-adjective-scores}\n",
    "report = []\n",
    "for title, emb in embeddings.items():\n",
    "\n",
    "    ### Prepare the dataset\n",
    "    # Create the training and testing dataset based on the given embedding:\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "\n",
    "    for adj, noun, is_met, _ in phrase_annotate:\n",
    "        X_all.append([emb[adj], emb[noun]])\n",
    "        y_all.append(is_met)\n",
    "\n",
    "    X_all = np.array(X_all)\n",
    "    y_all = np.array(y_all)\n",
    "\n",
    "    # split in half for train and test:\n",
    "    test_split = 500 #int(len(phrase_annotate)/2)\n",
    "    X_train, y_train = X_all[:test_split], y_all[:test_split]\n",
    "    X_test, y_test   = X_all[test_split:], y_all[test_split:]\n",
    "    \n",
    "    \n",
    "    ### Define the network layers\n",
    "    # Compose two vectors (W)\n",
    "    model_composer = Sequential()\n",
    "    model_composer.add(Dense(300, activation='linear',input_shape=(600,)))\n",
    "\n",
    "    # Map it to one measure (find a vector which maximized the prediction of metaphor) (q)\n",
    "    model_decoder = Sequential()\n",
    "    model_decoder.add(Dense(1, activation='sigmoid', input_shape=(300,)))\n",
    "\n",
    "    # Connecting models\n",
    "    input_adj  = Input(shape=(300,))\n",
    "    input_noun = Input(shape=(300,))\n",
    "    input_seq  = merge([input_adj, input_noun], mode='concat', concat_axis=1)\n",
    "    \n",
    "    out_binary = model_decoder(\n",
    "        model_composer(input_seq)\n",
    "    )\n",
    "\n",
    "    # final model specifications (loss, optimizer, and etc.)\n",
    "    final_model = Model(input=[input_adj, input_noun], output=out_binary)\n",
    "    final_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy', #good\n",
    "                  #loss='mse', #good \n",
    "                  #loss='msle', #mehhh\n",
    "                  #loss='cosine_proximity', #nope\n",
    "                  metrics=['accuracy', 'recall', 'precision'])\n",
    "\n",
    "    ### Train the network\n",
    "    final_model.fit([X_train[:,0], X_train[:,1]], y_train, nb_epoch=20, batch_size=100, validation_split=0.0)\n",
    "\n",
    "    \n",
    "    ### Evaluate the trained network based on the test data\n",
    "    score = final_model.evaluate([X_test[:,0], X_test[:,1]], y_test, batch_size=len(X_test))\n",
    "    \n",
    "    # print and save the report\n",
    "    print(\"\\n\")\n",
    "    print(\"Embedding:\", title)\n",
    "    for key, value in dict(zip(final_model.metrics_names, score)).items():\n",
    "        print(\"{0:10} {1:0.4}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secod architecture\n",
    "\n",
    "This architecture is even more simpler than the first one. In this model, the weight matrix is shared between noun  and adjective:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{p} = f_{\\theta}(\\mathbf{u}, \\mathbf{v}) = \\mathbf{u}W + \\mathbf{v} W + b\n",
    "\\end{equation}\n",
    "\n",
    "Notice that in case of comparing two compositions $b$ is redundant. \n",
    "A benefit of this model is that with the same accuracy the trained transformation function can map any word vector weather it's in compositional relation into the new vector space. In the new vector space addition of any two vector can be compared with metaphoricity vector. \n",
    "\n",
    "In this new vector space, a simple addition operator compose two vectors. The resulted vector is comparable to metaphor vecotor $mathbfq$ to predict the metaphoricity of composition based on hypothesis on metaphoricity of adjective-nouns in Gutierrez et al. (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 0s - loss: 0.6166 - acc: 0.6740 - recall: 0.5650 - precision: 0.7464     \n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 0s - loss: 0.4160 - acc: 0.8720 - recall: 0.8803 - precision: 0.8715     \n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 0s - loss: 0.3137 - acc: 0.8900 - recall: 0.8707 - precision: 0.9057     \n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 0s - loss: 0.2501 - acc: 0.9060 - recall: 0.8949 - precision: 0.9188     \n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 0s - loss: 0.2098 - acc: 0.9300 - recall: 0.9447 - precision: 0.9188     \n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 0s - loss: 0.1763 - acc: 0.9420 - recall: 0.9606 - precision: 0.9279     \n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 0s - loss: 0.1522 - acc: 0.9520 - recall: 0.9620 - precision: 0.9456     \n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 0s - loss: 0.1335 - acc: 0.9640 - recall: 0.9726 - precision: 0.9568     \n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 0s - loss: 0.1165 - acc: 0.9700 - recall: 0.9885 - precision: 0.9558     \n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 0s - loss: 0.1051 - acc: 0.9740 - recall: 0.9852 - precision: 0.9665     \n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 0s - loss: 0.0948 - acc: 0.9780 - recall: 0.9879 - precision: 0.9669     \n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 0s - loss: 0.0855 - acc: 0.9840 - recall: 0.9962 - precision: 0.9740     \n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 0s - loss: 0.0789 - acc: 0.9900 - recall: 0.9905 - precision: 0.9872     \n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 0s - loss: 0.0708 - acc: 0.9940 - recall: 0.9960 - precision: 0.9922     \n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 0s - loss: 0.0655 - acc: 0.9920 - recall: 0.9964 - precision: 0.9870     \n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 0s - loss: 0.0595 - acc: 0.9940 - recall: 0.9960 - precision: 0.9921     \n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 0s - loss: 0.0551 - acc: 0.9940 - recall: 0.9959 - precision: 0.9922     \n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 0s - loss: 0.0495 - acc: 0.9940 - recall: 0.9959 - precision: 0.9914     \n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 0s - loss: 0.0455 - acc: 0.9940 - recall: 0.9964 - precision: 0.9922     \n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 0s - loss: 0.0430 - acc: 0.9940 - recall: 0.9963 - precision: 0.9919     \n",
      "7992/7992 [==============================] - 0s\n",
      "\n",
      "\n",
      "Embedding: w2v-gnews\n",
      "acc        0.9334\n",
      "recall     0.9384\n",
      "loss       0.1829\n",
      "precision  0.9376\n"
     ]
    }
   ],
   "source": [
    "# First choose an embedding for this part\n",
    "# embeding {title, total-score, per-adjective-scores}\n",
    "report = []\n",
    "for title, emb in embeddings.items():\n",
    "\n",
    "    ### Prepare the dataset\n",
    "    # Create the training and testing dataset based on the given embedding:\n",
    "    X_all = []\n",
    "    y_all = []\n",
    "\n",
    "    for adj, noun, is_met, _ in phrase_annotate:\n",
    "        X_all.append([emb[adj], emb[noun]])\n",
    "        y_all.append(is_met)\n",
    "\n",
    "    X_all = np.array(X_all)\n",
    "    y_all = np.array(y_all)\n",
    "\n",
    "    # split in half for train and test:\n",
    "    test_split = 500 #int(len(phrase_annotate)/2)\n",
    "    X_train, y_train = X_all[:test_split], y_all[:test_split]\n",
    "    X_test, y_test   = X_all[test_split:], y_all[test_split:]\n",
    "    \n",
    "    \n",
    "    ### Define the network layers\n",
    "    # Compose two vectors (W)\n",
    "    model_composer = Sequential()\n",
    "    model_composer.add(Dense(300, activation='linear',input_shape=(300,)))\n",
    "\n",
    "    # Map it to one measure (find a vector which maximized the prediction of metaphor) (q)\n",
    "    model_decoder = Sequential()\n",
    "    model_decoder.add(Dense(1, activation='sigmoid', input_shape=(300,)))\n",
    "\n",
    "    # Connecting models\n",
    "    input_adj  = Input(shape=(300,))\n",
    "    input_noun = Input(shape=(300,))\n",
    "    input_seq  = merge([input_adj, input_noun], mode='sum', concat_axis=1)\n",
    "    \n",
    "    out_binary = model_decoder(\n",
    "        model_composer(input_seq)\n",
    "    )\n",
    "\n",
    "    # final model specifications (loss, optimizer, and etc.)\n",
    "    final_model = Model(input=[input_adj, input_noun], output=out_binary)\n",
    "    final_model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy', #good\n",
    "                  #loss='mse', #good \n",
    "                  #loss='msle', #mehhh\n",
    "                  #loss='cosine_proximity', #nope\n",
    "                  metrics=['accuracy', 'recall', 'precision'])\n",
    "\n",
    "    ### Train the network\n",
    "    final_model.fit([X_train[:,0], X_train[:,1]], y_train, nb_epoch=20, batch_size=100, validation_split=0.0)\n",
    "\n",
    "    \n",
    "    ### Evaluate the trained network based on the test data\n",
    "    score = final_model.evaluate([X_test[:,0], X_test[:,1]], y_test, batch_size=len(X_test))\n",
    "    \n",
    "    # print and save the report\n",
    "    print(\"\\n\")\n",
    "    print(\"Embedding:\", title)\n",
    "    for key, value in dict(zip(final_model.metrics_names, score)).items():\n",
    "        print(\"{0:10} {1:0.4}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metaphoricity vector\n",
    "\n",
    "\n",
    "As the objective function is to minimize the distance between the predicted metaphoricity and gold standard in dataset which is one of the values $\\{0, 1\\}$ in our training data, the final layer of parameters can be considered as indicator vector for maximum metaphoricity:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{p} = f_{\\theta}(\\mathbf{u}, \\mathbf{v}) = \\mathbf{u}W + \\mathbf{v} W + b_0 \\\\\n",
    "\\hat{y} = \\sigma(\\mathbf{p} \\cdotp \\mathbf{q} + b_1) = \\frac{1}{1+e^{-\\mathbf{p} \\cdotp \\mathbf{q} + b_1}}\n",
    "\\end{equation}\n",
    "\n",
    "where, $b_1$ is the regularization for final layer, $\\mathbf{q}$ as metaphoricity indicator, and $\\hat{y}$ is the predicted score of metaphoricity.\n",
    "\n",
    "We fit the $\\theta$ with our supervised data using adam stochastic gradient descent with training size of $T=500$: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{array}{r c l l}\n",
    "        \\mathbf{x} &=& (x_1, ... x_T) & \\text{adjective and nouns in training dataset}\\\\\n",
    "        \\mathbf{y} &=& (y_1, ... y_T) & \\text{labels training dataset}\\\\\n",
    "        \\theta &=& (W, b_0, \\mathbf{q}, b_1) & \\\\\n",
    "        P(\\mathbf{x}\\ \\text{are_all_metaphorical}) &=& \\prod_{t=1}^{T}{P(x_t\\ \\text{is_metaphorical})}& \\\\\n",
    "        y_t &=& P(x_t\\ \\text{is_metaphorical}) & \\in \\{0,1\\}\\\\\n",
    "        \\hat{y}_t & = & \\sigma(\\mathbf{p}_t \\cdotp \\mathbf{q} + b_1) &\\in (0, 1)  \\\\\n",
    "        \\mathcal{L}(\\mathbf{x}) &=& -\\sum_{t=1}^{T}{y_t \\mathrm{log}(\\hat{y}_t)+(1-y_t) \\mathrm{log}(1-\\hat{y}_t)} & \n",
    "    \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "where, each $x_t$ is a pair of adjective-noun vectors: $\\mathbf{u}$ and $\\mathbf{v}$ in previous equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
